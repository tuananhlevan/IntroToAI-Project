\documentclass{article}  % or use 'article' or 'book'

\usepackage[final]{neurips_2023}

% ---------- Packages ----------
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{graphicx}

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\title{Improve Predictive Model Calibration for Safety-Critical Domains}

\author{%
	Le Van Tuan Anh\thanks{Group 9}\\
	\texttt{anh.lvt2416657@sis.hust.edu.vn} \\
	% examples of more authors
	\And
	Nguyen Tran Minh Tri\samethanks \\
	\texttt{tri.ntm2400116@sis.hust.edu.vn} \\
	\AND
	Le Xuan Nhat Khoi\samethanks \\
	\texttt{khoi.lxn2416711@sis.hust.edu.vn} \\
	\And
	Nguyen Duy Tung\samethanks \\
	\texttt{tung.nd2416762@sis.hust.edu.vn} \\
}


\begin{document}
	% ---------- Title Page ----------
	\maketitle
	
	\section{Introduction}
	Deep neural networks have achieved strong performance on classification tasks. However, their predictive probabilities are often poorly calibrated, meaning the confidence scores do not reliably reflect true correctness likelihood. This limitation is particularly problematic in high-stakes domains, such as medical image analysis, where overconfident but incorrect predictions may lead to severe consequences.
	
	Traditional deterministic neural networks typically provide point estimates without meaningful uncertainty information. As a result, they are unable to distinguish between confident and uncertain predictions, especially under data ambiguity, limited training samples, or domain shift. This limitation motivates the use of Bayesian and approximate Bayesian methods to capture predictive uncertainty and improve model reliability.
	
	In this project, we focus on uncertainty-aware deep learning methods, specifically Monte Carlo Dropout (MC Dropout) and Bayesian Variational Inference (Bayesian VI), as scalable approximations to Bayesian neural networks. These methods allow uncertainty estimation without substantially changing the underlying model architecture or incurring prohibitive computational cost.
	
	The goals of this project are threefold:
	\begin{itemize}
		\item Evaluate the ability of MC Dropout and Bayesian Variational Inference to capture predictive uncertainty and improve model calibration on standard image classification benchmarks, namely CIFAR-10 and CIFAR-100.
		
		\item Compare uncertainty-aware models against deterministic baselines using calibration-focused metrics (e.g., Expected Calibration Error) and reliability visualizations, in addition to standard accuracy-based performance.
		
		\item Apply the studied uncertainty estimation methods to the task of brain tumor classification from medical images, assessing whether improved calibration and uncertainty awareness translate to more reliable predictions in a safety-critical setting.
	\end{itemize}
	Overall, this project aims to demonstrate that incorporating uncertainty estimation into deep learning models can enhance not only predictive reliability but also practical applicability, particularly in domains where decision confidence is as important as raw accuracy.
	
	\section{Conclusion}
	
	
\end{document}