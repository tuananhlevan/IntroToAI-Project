\documentclass{article}  % or use 'article' or 'book'

\usepackage[final]{neurips_2023}

% ---------- Packages ----------
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathrsfs}

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\title{Improve Predictive Model Calibration for Safety-Critical Domains}

\author{%
	Le Van Tuan Anh\thanks{DS-AI 03 K69}\\
	\texttt{anh.lvt2416657@sis.hust.edu.vn} \\
	% examples of more authors
	\And
	Nguyen Tran Minh Tri\samethanks \\
	\texttt{tri.ntm2400116@sis.hust.edu.vn} \\
	\AND
	Le Xuan Nhat Khoi\samethanks \\
	\texttt{khoi.lxn2416711@sis.hust.edu.vn} \\
	\And
	Nguyen Duy Tung\samethanks \\
	\texttt{tung.nd2416762@sis.hust.edu.vn} \\
}


\begin{document}
	% ---------- Title Page ----------
	\maketitle
	
	\begin{abstract}
		Reliable uncertainty estimation is crucial for deploying modern neural networks in high-stakes scenarios, yet standard deterministic models trained with conventional loss functions often become systematically overconfident in their predictions. In this work, we propose a Bayesian neural network (BNN) framework that improves model calibration by replacing traditional point-estimate training with optimization of the evidence lower bound (ELBO). Our approach learns a distribution over network weights, enabling principled uncertainty quantification and mitigating the overconfidence commonly induced by cross-entropy–based training. Empirical results demonstrate that our BNN formulation yields significantly better calibration, measured via expected calibration error and negative log-likelihood, while maintaining competitive predictive accuracy.
	\end{abstract}
	
	\section{Introduction}
	Neural networks have achieved remarkable performance across a wide range of predictive tasks, yet their reliability remains a significant challenge in practical deployment. Standard deterministic models trained with conventional loss functions, most notably cross-entropy, tend to produce overly confident predictions, even when faced with ambiguous inputs or samples far outside the training distribution. This systematic overconfidence not only impairs decision-making but also limits the applicability of modern deep learning systems in safety-critical domains such as medical diagnosis, autonomous navigation, and scientific modeling.
	
	Bayesian neural networks (BNNs) offer a principled framework for addressing this limitation by replacing point estimates of network weights with full posterior distributions. Instead of committing to a single set of parameters, BNNs represent uncertainty directly through the variability of sampled weight configurations, enabling more calibrated predictive distributions. However, training BNNs remains challenging due to the intractability of the exact posterior, which necessitates efficient and scalable approximations.
	
	In this work, we revisit variational Bayesian learning as a practical solution to these challenges. We propose a BNN framework trained via maximization of the evidence lower bound (ELBO), which balances model fit with posterior regularization through a Kullback–Leibler divergence term. Unlike traditional losses that focus solely on maximizing predictive accuracy, the ELBO encourages the model to capture epistemic uncertainty by constraining the posterior to remain close to a meaningful prior. This leads to better uncertainty estimates, improved robustness to distribution shift, and reduced overconfidence in regions where data are sparse or ambiguous.
	
	To assess the effectiveness of our approach, we conduct extensive empirical evaluations across classification tasks and calibration benchmarks. Our results show that ELBO-based training consistently improves calibration metrics, including expected calibration error and negative log-likelihood, while maintaining competitive accuracy. These findings highlight the value of Bayesian training objectives in producing trustworthy and informative predictions.
	
	\section{Bayesian Inference}
	Bayesian inference provides a principled mathematical framework for reasoning under uncertainty by treating model parameters as random variables rather than fixed quantities. Given observed data $\mathcal{D} = \{(x_{i}, y_{i})\}_{i = 1}^{N}$ and model parameter $\theta$, Bayes' rule defines the posterior distribution as:
	\[
	p(\theta \mid \mathcal{D}) = \frac{p(\theta)p(\mathcal{D} \mid \theta)}{p(\mathcal{D})}
	\]
	where $p(\theta)$ is a prior encoding assumptions about the parameters, $p(\mathcal{D} \mid \theta)$ is the likelihood of the data under the model, and $p(\mathcal{D})$ is the marginal likelihood (the evidence). The posterior captures all uncertainty about the parameters after observing data, and predictions are obtained by marginalizing over this uncertainty:
	\[
	p(y^{*} \mid x^{*}, \mathcal{D}) = \int p(y^{*} \mid x^{*}, \theta)p(\theta \mid \mathcal{D})d\theta
	\]
	
	\subsection{Bayesian Neural Networks}
	A Bayesian Neural Network (BNN) applies Bayesian inference to deep learning by placing distributions over the network weights instead of learning deterministic parameters. Let $\theta$ represent all weights and biases of the network. In a BNN, we specify:
	\begin{itemize}
		\item A \textbf{prior} $p(\theta)$, typically chosen as a factorized Gaussian with zero mean
		\item A \textbf{likelihood} $p(\mathcal{D} \mid \theta)$, which depends on the model output (e.g., softmax likelihood for classification)
	\end{itemize}
	Because the neural network likelihood is highly nonlinear and the parameter space is high-dimensional, the posterior is \textbf{intractable}, we cannot compute it analytically or normalize it exactly. Likewise, exact predictive inference requires evaluating an integral over all possible weight values, which is computationally prohibitive.
	
	\subsection{Probabilistic Inference}
	To address this intractability, BNNs rely on approximate inference methods. The two primary families are:
	\begin{itemize}
		\item \textbf{Markov chain Monte Carlo (MCMC)}: Draws samples from the true posterior using iterative sampling schemes (e.g., Hamiltonian Monte Carlo). While theoretically accurate, MCMC scales poorly with deep architectures and large datasets.
		\item \textbf{Variational Inference (VI)}: Approximates the posterior by a simpler distribution $q(\theta)$ and selects $q(\theta)$ to minimize the divergence from the true posterior. VI is computationally efficient, scalable to large datasets, and compatible with gradient-based optimization.
	\end{itemize}
	Variational inference forms the foundation of most practical BNN implementations, as it enables amortized training through stochastic optimization and mini-batching.
	
	\subsection{Predictive Uncertainty}
	BNNs provide two forms of uncertainty:
	\begin{itemize}
		\item \textbf{Epistemic Uncertainty}: Arising from limited data or model ambiguity, captured through variability in $p(\theta \mid \mathcal{D})$
		\item \textbf{Aleatoric Uncertainty}: Representing inherent noise in the data, captured through the likelihood $p(y \mid x, \theta)$
	\end{itemize}
	Integrating over the weight posterior naturally combines both sources:
	\[
	\mathbb{E}_{\theta \sim q}[p(y^{*} \mid x^{*}, \theta)]
	\]
	
	\section{Variational Inference and the Evidence Lower Bound}
	As discussed earlier, exact Bayesian inference in neural networks is intractable due to the high-dimensional and nonlinear nature of the weight posterior $p(\theta \mid \mathcal{D})$. Variational Inference (VI) provides a scalable alternative by reframing posterior inference as an optimization problem. Instead of computing the exact posterior, VI introduces a tractable family of distributions $q(\theta)$ and finds the member of this family that best approximates the true posterior.
	
	\subsection{Variational Approximation}
	We choose a variational distribution $q(\theta)$, typically a factorized Gaussian over weights:
	\[
	q(\theta) = \prod_{j} \mathcal{N}(\theta_{j} \mid \mu_{j}, \sigma^{2}_{j})
	\]
	The goal is to make $q(\theta)$ close to the true posterior. VI does this by minimizing the reverse	 Kullback–Leibler divergence:
	\[
	q^{\star} = \arg\min \mathbb{K}\mathbb{L}[q \parallel p(\cdot \mid \mathcal{D})]
	\]
	However, the posterior $p(\theta \mid \mathcal{D})$ is inaccessible, so this KL divergence cannot be evaluated directly. Instead, we rewrite it in a tractable form.
	
	\subsection{Deriving the ELBO}
	We begin with the reverse KL-divergence:
	\begin{align*}
		\mathbb{K}\mathbb{L}[q(\theta) \parallel p(\theta \mid \mathcal{D})]
		&= \mathbb{E}_{\theta \sim q} \left[\log \frac{q(\theta)}{p(\theta \mid \mathcal{D})} \right] \\
		&= \mathbb{E}_{\theta \sim q} \left[ \log \frac{p(y_{1:n} \mid x_{1:n})q(\theta)}{p(y_{1:n}, \theta \mid x_{1:n})} \right] \\
		&= \log p(y_{1:n} \mid x_{1:n}) \underbrace{-\mathbb{E}_{\theta \sim q}[\log p(y_{1:n}, \theta \mid x_{1:n})] - \mathrm{H}[q]}_{-\mathcal{L}(q, p; \mathcal{D}_n)}
	\end{align*}
	
	Because KL divergence is always non-negative, the ELBO forms a lower bound:
	\[
	\mathcal{L}(q, p; \mathcal{D}_{n}) \leq \log p(y_{1:n} \mid x_{1:n})
	\]
	Thus maximizing the ELBO is equivalent to minimizing the reverse KL divergence.
	We can also express the ELBO in other forms:
	\begin{align*}
		\mathcal{L}(q, p; \mathcal{D}_{n})
		&= \mathbb{E}_{\theta \sim q}\left[ \log p(y_{1:n}, \theta \mid x_{1:n}) - \log q(\theta) \right] \\
		&= \mathbb{E}_{\theta \sim q}\left[ \log p(y_{1:n} \mid x_{1:n}, \theta) + \log p(\theta) - \log q(\theta) \right] \\
		&= \mathbb{E}_{\theta \sim q}\left[ \log p(y_{1:n} \mid x_{1:n}, \theta) \right] - \mathbb{K}\mathbb{L}[q \parallel p(\cdot)]
	\end{align*}
	
	\subsection{Stochastic Optimization and the Reparameterization Trick}
	Computing the gradients of the ELBO requires sampling from $q(\theta)$. VI employs the \textbf{reparameterization trick} to make this sampling differentiable:
	\[
	\theta = \mu + \sigma \odot \epsilon
	\]
	This transforms stochastic sampling into a deterministic function of $\theta$ and allows backpropagation through expectations:
	\[
	\nabla \mathbb{E} \left[f(\theta)\right] = \mathbb{E}_{\epsilon \sim \mathcal{N}(0, \mathit(I))}\left[\nabla f(\mu + \sigma \odot \epsilon)\right]
	\]
	As a result, ELBO maximization can be done efficiently using SGD or Adam, making variational BNNs scalable to large datasets.
	
	\subsection{Why ELBO Improves Calibration}
	Traditional losses such as cross-entropy encourage the logits of the predictive distribution to grow without bound, often producing overconfident outputs even for uncertain inputs.
	
	In contrast, ELBO-based training:
	\begin{itemize}
		\item Enforces a distribution over weights, naturally reflecting epistemic uncertainty
		\item Constrains the posterior through the KL term, preventing extreme parameter values
		\item Averages predictions over multiple weight samples, smoothing the predictive distribution
		\item Penalizes overconfident predictions that are unsupported by data
	\end{itemize}
	This results in models that know when they don’t know, leading to better-calibrated probabilities and more trustworthy predictions.
	
	\section{Architechture}
	Our Bayesian neural network framework builds upon a standard feed-forward architecture, but replaces deterministic weights with variational distributions. This section describes the overall model structure, the parameterization of the variational layers, and the modifications required to support ELBO-based training.
	
	\subsection{Overall Structure}
	The proposed model follows a conventional multilayer perceptron (MLP) architecture composed of $L$ hidden layers:
	\[
	f_{\theta} = f^{(L)} \circ f^{(L - 1)} \circ \dots \circ f^{(1)}(x)
	\]
	where each layer consists of an affine transformation followed by a nonlinear activation.
	
	In the deterministic case, each layer uses weights $W^{(\ell)}$ and biases $b^{(\ell)}$.
	
	In our Bayesian formulation, these parameters are replaced by distributions:
	\[
	W^{(\ell)} \sim q(W^{(\ell)}), \qquad b^{(\ell)} \sim q(b^{(\ell)})
	\]
	The output layer uses a task-appropriate likelihood:
	\begin{itemize}
		\item \textbf{Softmax likelihood} for classification
		\item \textbf{Gaussian likelihood} for regression
	\end{itemize}
	
	\subsection{Variational (Bayesian) Layers}
	Each weight tensor $W^{(\ell)}$ is modelled using a mean-field Gaussian distribution is modeled using a mean-field Gaussian distribution:
	\[
	q(W^{(\ell)}) = \mathcal{N}\left(W^{(\ell)} \mid \mu_{W}^{(\ell)}, \sigma_{W}^{(\ell)2}\right)
	\]
	and similarly for $b^{(\ell)}$. The parameter $\{ \mu, \sigma \}$ are optimized during training.
	
	During forward passes, weight samples are generated using the reparameterization trick:
	\[
	W^{(\ell)} = \mu_{W}^{(\ell)} + \sigma_{W}^{(\ell)} \odot \epsilon^{(\ell)}, \quad \epsilon^{(\ell)} \sim \mathcal{N}(0, \textit{I})
	\]
	Thus each model evaluation corresponds to a single draw from the approximate posterior.
	
	\subsection{Forward Pass and Monte Carlo Predictive Distribution}
	A single forward pass yields a stochastic prediction due to sampling from the posterior.
	
	For calibrated predictions, we compute the predictive distribution by averaging over multiple samples:
	\[
	p(y^{*} \mid x^{*}) \approx \frac{1}{S}\sum_{s = 1}^{S}p(y^{*} \mid x^{*}, \theta_{s})
	\]
	In practice, using $S = 10$ - $20$ balances predictive stability and runtime.
	
	\section{Experiments}
	
	
	\section{Conclusion}
	
	
\end{document}